{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/opt/manual/spark/\")\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2022-02-12 20:41:14,367 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-02-12 20:41:15,803 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".appName(\"ML Homework\") \\\n",
    ".master(\"local[2]\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".load(\"file:///home/train/datasets/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.withColumn(\"pickup_datetime\", F.to_timestamp(F.col(\"pickup_datetime\"),\"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "        .withColumn(\"dropoff_datetime\", F.to_timestamp(F.col(\"dropoff_datetime\"),\"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1 \\\n",
    "       .withColumn(\"pickup_year\",\n",
    "                    F.year(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_month\",\n",
    "                    F.month(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_dayofweek\",\n",
    "                    F.dayofweek(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_hour\",\n",
    "                    F.hour(F.col(\"pickup_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_tr_day(day_index):\n",
    "    my_dict = {\n",
    "        1: 'Pazar',\n",
    "        2: 'Pazartesi',\n",
    "        3: 'Salı',\n",
    "        4: 'Çarşamba',\n",
    "        5: 'Perşembe',\n",
    "        6: 'Cuma',\n",
    "        7: 'Cumartesi'\n",
    "    }\n",
    "    \n",
    "    return my_dict.get(day_index)\n",
    "\n",
    "def switch_month_day(month_index):\n",
    "    my_dict = {\n",
    "        1: 'Ocak',\n",
    "        2: 'Subat',\n",
    "        3: 'Mart',\n",
    "        4: 'Nisan',\n",
    "        5: 'Mayis',\n",
    "        6: 'Haziran',\n",
    "        7: 'Temmuz',\n",
    "        8: 'Agustos',\n",
    "        9: 'Eylul',\n",
    "        10: 'Ekim',\n",
    "        11: 'Kasim',\n",
    "        12: 'Aralik'\n",
    "    }\n",
    "    \n",
    "    return my_dict.get(month_index)\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, FloatType\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "haversine_distance = F.udf(lambda lon1, lat1, lon2, lat2: haversine(lon1, lat1, lon2, lat2), FloatType())\n",
    "spark.udf.register(\"haversine_distance\", haversine_distance)\n",
    "\n",
    "switch_month = F.udf(lambda z: switch_month_day(z), StringType())\n",
    "spark.udf.register(\"switch_month\", switch_month)\n",
    "\n",
    "switch_day_func = F.udf(lambda z: switch_tr_day(z), StringType())\n",
    "spark.udf.register(\"switch_day_func\", switch_day_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>pickup_dayofweek</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>haversine_distance_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>455</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1.498521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>663</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.805507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6.385098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1.485498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>435</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1.188589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_id  passenger_count  trip_duration  pickup_month  pickup_dayofweek  \\\n",
       "0          2                1            455             3                 2   \n",
       "1          1                1            663             6                 1   \n",
       "2          2                1           2124             1                 3   \n",
       "3          2                1            429             4                 4   \n",
       "4          2                1            435             3                 7   \n",
       "\n",
       "   pickup_hour  haversine_distance_km  \n",
       "0           17               1.498521  \n",
       "1            0               1.805507  \n",
       "2           11               6.385098  \n",
       "3           19               1.485498  \n",
       "4           13               1.188589  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2.withColumn(\"id\", F.regexp_replace(F.col(\"id\"), \"[id]\", \"\")) \\\n",
    ".withColumn(\"haversine_distance_km\", haversine_distance(F.col(\"pickup_longitude\"), F.col(\"pickup_latitude\"), F.col(\"dropoff_longitude\"), F.col(\"dropoff_latitude\"))) \\\n",
    ".drop(\"store_and_fwd_flag\", \"pickup_year\", \"dropoff_datetime\", \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"id\")\n",
    "\n",
    "df3.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>label</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>pickup_dayofweek</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>haversine_distance_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>455</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1.498521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>663</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.805507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6.385098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1.485498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>435</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1.188589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>443</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>1.098943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>341</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1.326279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1551</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5.714981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1.310353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1225</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.121161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_id  passenger_count  label  pickup_month  pickup_dayofweek  \\\n",
       "0          2                1    455             3                 2   \n",
       "1          1                1    663             6                 1   \n",
       "2          2                1   2124             1                 3   \n",
       "3          2                1    429             4                 4   \n",
       "4          2                1    435             3                 7   \n",
       "5          2                6    443             1                 7   \n",
       "6          1                4    341             6                 6   \n",
       "7          2                1   1551             5                 7   \n",
       "8          1                1    255             5                 6   \n",
       "9          2                1   1225             3                 5   \n",
       "\n",
       "   pickup_hour  haversine_distance_km  \n",
       "0           17               1.498521  \n",
       "1            0               1.805507  \n",
       "2           11               6.385098  \n",
       "3           19               1.485498  \n",
       "4           13               1.188589  \n",
       "5           22               1.098943  \n",
       "6           22               1.326279  \n",
       "7            7               5.714981  \n",
       "8           23               1.310353  \n",
       "9           21               5.121161  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4 = df3.withColumnRenamed(\"trip_duration\", \"label\")\n",
    "df4.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['vendor_id', 'passenger_count', 'pickup_month', 'pickup_dayofweek',\n",
    "            'pickup_hour', 'haversine_distance_km'], outputCol='assembled_features', \n",
    "            handleInvalid='skip')\n",
    "\n",
    "df5 = assembler.setHandleInvalid(\"skip\").transform(df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol='assembled_features',\n",
    "    outputCol='features')\n",
    "\n",
    "scaler_model = scaler.fit(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>label</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>pickup_dayofweek</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>haversine_distance_km</th>\n",
       "      <th>assembled_features</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>455</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1.498521</td>\n",
       "      <td>[2.0, 1.0, 3.0, 2.0, 17.0, 1.4985207319259644]</td>\n",
       "      <td>[4.009806753040021, 0.7608947760794935, 1.7846...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>663</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.805507</td>\n",
       "      <td>[1.0, 1.0, 6.0, 1.0, 0.0, 1.8055071830749512]</td>\n",
       "      <td>[2.0049033765200104, 0.7608947760794935, 3.569...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2124</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6.385098</td>\n",
       "      <td>[2.0, 1.0, 1.0, 3.0, 11.0, 6.385098457336426]</td>\n",
       "      <td>[4.009806753040021, 0.7608947760794935, 0.5948...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>429</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1.485498</td>\n",
       "      <td>[2.0, 1.0, 4.0, 4.0, 19.0, 1.4854984283447266]</td>\n",
       "      <td>[4.009806753040021, 0.7608947760794935, 2.3794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>435</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>1.188589</td>\n",
       "      <td>[2.0, 1.0, 3.0, 7.0, 13.0, 1.1885885000228882]</td>\n",
       "      <td>[4.009806753040021, 0.7608947760794935, 1.7846...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor_id  passenger_count  label  pickup_month  pickup_dayofweek  \\\n",
       "0          2                1    455             3                 2   \n",
       "1          1                1    663             6                 1   \n",
       "2          2                1   2124             1                 3   \n",
       "3          2                1    429             4                 4   \n",
       "4          2                1    435             3                 7   \n",
       "\n",
       "   pickup_hour  haversine_distance_km  \\\n",
       "0           17               1.498521   \n",
       "1            0               1.805507   \n",
       "2           11               6.385098   \n",
       "3           19               1.485498   \n",
       "4           13               1.188589   \n",
       "\n",
       "                               assembled_features  \\\n",
       "0  [2.0, 1.0, 3.0, 2.0, 17.0, 1.4985207319259644]   \n",
       "1   [1.0, 1.0, 6.0, 1.0, 0.0, 1.8055071830749512]   \n",
       "2   [2.0, 1.0, 1.0, 3.0, 11.0, 6.385098457336426]   \n",
       "3  [2.0, 1.0, 4.0, 4.0, 19.0, 1.4854984283447266]   \n",
       "4  [2.0, 1.0, 3.0, 7.0, 13.0, 1.1885885000228882]   \n",
       "\n",
       "                                            features  \n",
       "0  [4.009806753040021, 0.7608947760794935, 1.7846...  \n",
       "1  [2.0049033765200104, 0.7608947760794935, 3.569...  \n",
       "2  [4.009806753040021, 0.7608947760794935, 0.5948...  \n",
       "3  [4.009806753040021, 0.7608947760794935, 2.3794...  \n",
       "4  [4.009806753040021, 0.7608947760794935, 1.7846...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6 = scaler_model.transform(df5)\n",
    "\n",
    "df6.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import DecisionTreeRegressor, LinearRegression, RandomForestRegressor\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.sql import SparkSession, SQLContext, Row\n",
    "from pyspark.sql.functions import date_format, sin, cos, radians, atan2, month\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    " # Model\n",
    "dtr = RandomForestRegressor(featuresCol=\"features\", labelCol=\"label\", impurity=\"variance\")\n",
    "\n",
    "# choices of tuning parameters\n",
    "dtrparamGrid = (ParamGridBuilder()\n",
    "    .addGrid(dtr.numTrees, [10,20,50])\n",
    "    .addGrid(dtr.seed, [42])\n",
    "    .addGrid(dtr.maxDepth, [5,10]).build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "pipeline_obj = Pipeline(stages=[assembler, scaler, dtr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_model = pipeline_obj.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save(\"file:///home/train/project/nyc_taxi_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "loaded_pipeline_model = PipelineModel.load(\"file:///home/train/project/nyc_taxi_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline_obj_nolabel = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "pipeline_obj_nolabel_model = pipeline_obj_nolabel.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_obj_nolabel_model.write().overwrite().save(\"file:///home/train/project/nyc_taxi_model_nolabel_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-09 23:30:45,184 WARN execution.CacheManager: Asked to cache already cached data.\n",
      "2022-02-09 23:30:45,223 WARN execution.CacheManager: Asked to cache already cached data.\n",
      "[Stage 576:============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = [assembler, dtr])\n",
    "\n",
    "crossval = CrossValidator(estimator = pipeline, estimatorParamMaps = dtrparamGrid, evaluator = RegressionEvaluator(labelCol = \"label\", predictionCol = \"prediction\", metricName = \"rmse\"), numFolds = 10)\n",
    "model = crossval.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 569:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "|vendor_id|passenger_count|label|pickup_month|pickup_dayofweek|pickup_hour|haversine_distance_km|            features|        prediction|\n",
      "+---------+---------------+-----+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "|        2|              1|  455|           3|               2|         17|            1.4985207|[2.0,1.0,3.0,2.0,...| 780.4021992993382|\n",
      "|        1|              1|  663|           6|               1|          0|            1.8055072|[1.0,1.0,6.0,1.0,...| 551.2517193947731|\n",
      "|        2|              1| 2124|           1|               3|         11|            6.3850985|[2.0,1.0,1.0,3.0,...|1628.3537974683545|\n",
      "|        2|              1|  429|           4|               4|         19|            1.4854984|[2.0,1.0,4.0,4.0,...| 780.4021992993382|\n",
      "|        2|              1|  435|           3|               7|         13|            1.1885885|[2.0,1.0,3.0,7.0,...| 716.7535376404949|\n",
      "|        2|              6|  443|           1|               7|         22|            1.0989425|[2.0,6.0,1.0,7.0,...|             415.1|\n",
      "|        1|              4|  341|           6|               6|         22|            1.3262786|[1.0,4.0,6.0,6.0,...| 465.0740740740741|\n",
      "|        2|              1| 1551|           5|               7|          7|            5.7149806|[2.0,1.0,5.0,7.0,...| 1310.593555093555|\n",
      "|        1|              1|  255|           5|               6|         23|            1.3103533|[1.0,1.0,5.0,6.0,...| 465.0740740740741|\n",
      "|        2|              1| 1225|           3|               5|         21|            5.1211615|[2.0,1.0,3.0,5.0,...| 1500.750883808626|\n",
      "|        2|              1| 1274|           5|               3|         22|            3.8061395|[2.0,1.0,5.0,3.0,...| 969.7113133940182|\n",
      "|        2|              4| 1128|           5|               1|         11|            3.7730958|[2.0,4.0,5.0,1.0,...|1215.5655058043117|\n",
      "|        2|              2| 1114|           2|               6|          9|             1.859483|[2.0,2.0,2.0,6.0,...| 920.0371319226998|\n",
      "|        2|              1|  260|           6|               4|         20|           0.99168485|[2.0,1.0,6.0,4.0,...| 552.6265382259592|\n",
      "|        2|              1| 1414|           5|               6|          0|             6.382836|[2.0,1.0,5.0,6.0,...|1438.7661016949153|\n",
      "|        1|              1|  211|           5|               2|         15|             0.656578|[1.0,1.0,5.0,2.0,...| 289.4960309383269|\n",
      "|        2|              1| 2316|           4|               2|         17|             3.428086|[2.0,1.0,4.0,2.0,...|1259.9092154847551|\n",
      "|        1|              1|  731|           4|               5|          8|            2.5386717|[1.0,1.0,4.0,5.0,...| 957.5206775522662|\n",
      "|        1|              1| 1317|           6|               2|          9|            4.6052012|[1.0,1.0,6.0,2.0,...|1544.1713536354364|\n",
      "|        2|              1|  251|           6|               1|         13|            1.3032712|[2.0,1.0,6.0,1.0,...|  935.545051521687|\n",
      "|        1|              1|  486|           2|               1|          2|            2.5059261|[1.0,1.0,2.0,1.0,...| 598.6543385490754|\n",
      "|        2|              1|  652|           4|               6|         12|            1.7245501|[2.0,1.0,4.0,6.0,...| 920.0371319226998|\n",
      "|        1|              1|  423|           4|               7|          3|            2.0670848|[1.0,1.0,4.0,7.0,...| 445.2248764415157|\n",
      "|        1|              1| 1163|           6|               7|         10|            4.8747926|[1.0,1.0,6.0,7.0,...| 1070.339732888147|\n",
      "|        2|              1| 2485|           6|               6|          8|            20.602575|[2.0,1.0,6.0,6.0,...| 4295.779783393502|\n",
      "+---------+---------------+-----+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df4).cache()\n",
    "predictions.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 570:============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol = \"prediction\", metricName = \"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on train data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 571:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE) on test data = 422.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator2 = RegressionEvaluator(labelCol=\"label\", predictionCol = \"prediction\", metricName = \"mae\")\n",
    "mae = evaluator2.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on train data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = spark.read.format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".option(\"inferSchema\",True) \\\n",
    ".option(\"sep\",\",\") \\\n",
    ".load(\"file:///home/train/datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test.withColumn(\"pickup_datetime\", F.to_timestamp(F.col(\"pickup_datetime\"),\"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "test2 = test1 \\\n",
    "       .withColumn(\"pickup_year\",\n",
    "                    F.year(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_month\",\n",
    "                    F.month(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_dayofweek\",\n",
    "                    F.dayofweek(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "       .withColumn(\"pickup_hour\",\n",
    "                    F.hour(F.col(\"pickup_datetime\")))\n",
    "\n",
    "testdf = test2.withColumn(\"id\", F.regexp_replace(F.col(\"id\"), \"[id]\", \"\")) \\\n",
    ".withColumn(\"haversine_distance_km\", haversine_distance(F.col(\"pickup_longitude\"), F.col(\"pickup_latitude\"), F.col(\"dropoff_longitude\"), F.col(\"dropoff_latitude\"))) \\\n",
    ".drop(\"store_and_fwd_flag\", \"pickup_year\", \"pickup_datetime\", \"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+--------------------+------------------+\n",
      "|vendor_id|passenger_count|pickup_month|pickup_dayofweek|pickup_hour|haversine_distance_km|  assembled_features|            features|        prediction|\n",
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+--------------------+------------------+\n",
      "|        1|              1|           6|               5|         23|            2.7464259|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 849.5595431725029|\n",
      "|        1|              1|           6|               5|         23|             2.759239|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 849.5595431725029|\n",
      "|        1|              1|           6|               5|         23|            1.3061554|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 653.1291355384299|\n",
      "|        2|              1|           6|               5|         23|             5.269088|[2.0,1.0,6.0,5.0,...|[4.00980675304002...|1445.8212516475173|\n",
      "|        1|              1|           6|               5|         23|           0.96084183|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 641.4474156916042|\n",
      "|        1|              1|           6|               5|         23|            4.1862717|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 867.3819040855057|\n",
      "|        1|              1|           6|               5|         23|            3.5126143|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 867.3819040855057|\n",
      "|        1|              2|           6|               5|         23|            2.9809535|[1.0,2.0,6.0,5.0,...|[2.00490337652001...| 859.0569473592064|\n",
      "|        2|              2|           6|               5|         23|             18.85217|[2.0,2.0,6.0,5.0,...|[4.00980675304002...| 2250.681911185221|\n",
      "|        2|              1|           6|               5|         23|             1.820015|[2.0,1.0,6.0,5.0,...|[4.00980675304002...| 847.9049362990658|\n",
      "|        1|              4|           6|               5|         23|            4.7531476|[1.0,4.0,6.0,5.0,...|[2.00490337652001...|1101.6993277756035|\n",
      "|        2|              1|           6|               5|         23|            0.7883199|[2.0,1.0,6.0,5.0,...|[4.00980675304002...|  812.856381597416|\n",
      "|        1|              1|           6|               5|         23|            1.5244267|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 653.1291355384299|\n",
      "|        2|              1|           6|               5|         23|            2.6105702|[2.0,1.0,6.0,5.0,...|[4.00980675304002...|1031.7451681080433|\n",
      "|        1|              1|           6|               5|         23|             5.436126|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 1107.216575816286|\n",
      "|        1|              1|           6|               5|         23|            4.8071556|[1.0,1.0,6.0,5.0,...|[2.00490337652001...| 1107.216575816286|\n",
      "|        2|              1|           6|               5|         23|             3.589923|[2.0,1.0,6.0,5.0,...|[4.00980675304002...|1113.1119971911535|\n",
      "|        2|              5|           6|               5|         23|            1.7486739|[2.0,5.0,6.0,5.0,...|[4.00980675304002...| 873.9532534706739|\n",
      "|        2|              1|           6|               5|         23|           0.69044006|[2.0,1.0,6.0,5.0,...|[4.00980675304002...|  812.856381597416|\n",
      "|        2|              1|           6|               5|         23|            5.5612183|[2.0,1.0,6.0,5.0,...|[4.00980675304002...| 1710.609969465078|\n",
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "transformed_df = loaded_pipeline_model.transform(testdf)\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 568:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "|vendor_id|passenger_count|pickup_month|pickup_dayofweek|pickup_hour|haversine_distance_km|            features|        prediction|\n",
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "|        1|              1|           6|               5|         23|            2.7464259|[1.0,1.0,6.0,5.0,...| 773.8750799403113|\n",
      "|        1|              1|           6|               5|         23|             2.759239|[1.0,1.0,6.0,5.0,...| 773.8750799403113|\n",
      "|        1|              1|           6|               5|         23|            1.3061554|[1.0,1.0,6.0,5.0,...| 465.0740740740741|\n",
      "|        2|              1|           6|               5|         23|             5.269088|[2.0,1.0,6.0,5.0,...| 1500.750883808626|\n",
      "|        1|              1|           6|               5|         23|           0.96084183|[1.0,1.0,6.0,5.0,...| 336.6153426153426|\n",
      "|        1|              1|           6|               5|         23|            4.1862717|[1.0,1.0,6.0,5.0,...|1025.2129975124378|\n",
      "|        1|              1|           6|               5|         23|            3.5126143|[1.0,1.0,6.0,5.0,...| 931.7205463004652|\n",
      "|        1|              2|           6|               5|         23|            2.9809535|[1.0,2.0,6.0,5.0,...| 773.8750799403113|\n",
      "|        2|              2|           6|               5|         23|             18.85217|[2.0,2.0,6.0,5.0,...| 2352.992700729927|\n",
      "|        2|              1|           6|               5|         23|             1.820015|[2.0,1.0,6.0,5.0,...|  839.323492063492|\n",
      "|        1|              4|           6|               5|         23|            4.7531476|[1.0,4.0,6.0,5.0,...|1179.5168845315904|\n",
      "|        2|              1|           6|               5|         23|            0.7883199|[2.0,1.0,6.0,5.0,...| 552.6265382259592|\n",
      "|        1|              1|           6|               5|         23|            1.5244267|[1.0,1.0,6.0,5.0,...|  458.257761732852|\n",
      "|        2|              1|           6|               5|         23|            2.6105702|[2.0,1.0,6.0,5.0,...| 775.6911165444172|\n",
      "|        1|              1|           6|               5|         23|             5.436126|[1.0,1.0,6.0,5.0,...|1179.5168845315904|\n",
      "|        1|              1|           6|               5|         23|            4.8071556|[1.0,1.0,6.0,5.0,...|1179.5168845315904|\n",
      "|        2|              1|           6|               5|         23|             3.589923|[2.0,1.0,6.0,5.0,...|1225.4079880552445|\n",
      "|        2|              5|           6|               5|         23|            1.7486739|[2.0,5.0,6.0,5.0,...|  997.926456071076|\n",
      "|        2|              1|           6|               5|         23|           0.69044006|[2.0,1.0,6.0,5.0,...| 552.6265382259592|\n",
      "|        2|              1|           6|               5|         23|            5.5612183|[2.0,1.0,6.0,5.0,...|1631.1659350307286|\n",
      "|        2|              1|           6|               5|         23|           0.74096406|[2.0,1.0,6.0,5.0,...| 552.6265382259592|\n",
      "|        1|              1|           6|               5|         23|            1.9841377|[1.0,1.0,6.0,5.0,...| 653.0323670137911|\n",
      "|        2|              5|           6|               5|         23|            1.7041072|[2.0,5.0,6.0,5.0,...|  997.926456071076|\n",
      "|        2|              1|           6|               5|         23|            4.7438645|[2.0,1.0,6.0,5.0,...| 1500.750883808626|\n",
      "|        1|              1|           6|               5|         23|              5.24045|[1.0,1.0,6.0,5.0,...|1179.5168845315904|\n",
      "+---------+---------------+------------+----------------+-----------+---------------------+--------------------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(testdf).cache()\n",
    "predictions.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol = \"prediction\", metricName = \"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator2 = RegressionEvaluator(labelCol=\"label\", predictionCol = \"prediction\", metricName = \"mae\")\n",
    "mae = evaluator2.evaluate(predictions)\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"file:///home/train/project/nyc_taxi_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel\n",
    "\n",
    "loaded_pipeline_model = TrainValidationSplitModel.load(\"file:///home/train/project/nyc_taxi_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = loaded_pipeline_model.transform(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df= final_df.drop(\"assembled_features\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# postgresql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcUrl = \"jdbc:postgresql://localhost/traindb?user=train&password=Ankara06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write \\\n",
    ".mode(\"overwrite\") \\\n",
    ".jdbc(url=jdbcUrl,\n",
    "              table=\"final_df\", \n",
    "              mode=\"overwrite\", \n",
    "              properties={\"driver\": 'org.postgresql.Driver'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`psql -U train -d traindb`\n",
    "\n",
    "`select * from final_df limit 5;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f1184234711a9995bb2467cce7b552174d112ea8dbc6085e4c224c89a655f36"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
