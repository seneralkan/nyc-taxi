{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"/opt/manual/spark\")\n",
    "from pyspark.sql import SparkSession,functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/manual/spark-3.1.1-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/train/.ivy2/cache\n",
      "The jars for the packages stored in: /home/train/.ivy2/jars\n",
      "org.elasticsearch#elasticsearch-spark-30_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fc6320fd-fd88-44fa-90f8-ce4a7c57e207;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.elasticsearch#elasticsearch-spark-30_2.12;7.12.1 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.6 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.1 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in local-m2-cache\n",
      "\tfound org.apache.spark#spark-yarn_2.12;3.0.1 in central\n",
      ":: resolution report :: resolve 3100ms :: artifacts dl 53ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.1 from local-m2-cache in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-yarn_2.12;3.0.1 from central in [default]\n",
      "\torg.elasticsearch#elasticsearch-spark-30_2.12;7.12.1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.6 from local-m2-cache in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fc6320fd-fd88-44fa-90f8-ce4a7c57e207\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/96ms)\n",
      "2022-02-12 16:31:21,296 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Taxi Elasticsearch\")\n",
    "    .master(\"local[2]\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.jars.packages\", \"org.elasticsearch:elasticsearch-spark-30_2.12:7.12.1\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-14 17:24:55</td>\n",
       "      <td>2016-03-14 17:32:30</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-12 00:43:35</td>\n",
       "      <td>2016-06-12 00:54:38</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-01-19 11:35:24</td>\n",
       "      <td>2016-01-19 12:10:48</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-06 19:32:31</td>\n",
       "      <td>2016-04-06 19:39:40</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-03-26 13:30:55</td>\n",
       "      <td>2016-03-26 13:38:10</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id      pickup_datetime     dropoff_datetime  \\\n",
       "0  id2875421          2  2016-03-14 17:24:55  2016-03-14 17:32:30   \n",
       "1  id2377394          1  2016-06-12 00:43:35  2016-06-12 00:54:38   \n",
       "2  id3858529          2  2016-01-19 11:35:24  2016-01-19 12:10:48   \n",
       "3  id3504673          2  2016-04-06 19:32:31  2016-04-06 19:39:40   \n",
       "4  id2181028          2  2016-03-26 13:30:55  2016-03-26 13:38:10   \n",
       "\n",
       "   passenger_count  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0                1        -73.982155        40.767937         -73.964630   \n",
       "1                1        -73.980415        40.738564         -73.999481   \n",
       "2                1        -73.979027        40.763939         -74.005333   \n",
       "3                1        -74.010040        40.719971         -74.012268   \n",
       "4                1        -73.973053        40.793209         -73.972923   \n",
       "\n",
       "   dropoff_latitude store_and_fwd_flag  trip_duration  \n",
       "0         40.765602                  N            455  \n",
       "1         40.731152                  N            663  \n",
       "2         40.710087                  N           2124  \n",
       "3         40.706718                  N            429  \n",
       "4         40.782520                  N            435  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"file:///home/train/datasets/nyc_taxi.csv\")\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "subset = df.limit(2000)\n",
    "\n",
    "subset.write \\\n",
    ".format(\"csv\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".save(\"file:///home/train/datasets/taxi_subset_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, FloatType\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def switch_tr_day(day_index):\n",
    "    my_dict = {\n",
    "        1: 'Pazar',\n",
    "        2: 'Pazartesi',\n",
    "        3: 'Salı',\n",
    "        4: 'Çarşamba',\n",
    "        5: 'Perşembe',\n",
    "        6: 'Cuma',\n",
    "        7: 'Cumartesi'\n",
    "    }\n",
    "    \n",
    "    return my_dict.get(day_index)\n",
    "\n",
    "def switch_month_day(month_index):\n",
    "    my_dict = {\n",
    "        1: 'Ocak',\n",
    "        2: 'Subat',\n",
    "        3: 'Mart',\n",
    "        4: 'Nisan',\n",
    "        5: 'Mayis',\n",
    "        6: 'Haziran',\n",
    "        7: 'Temmuz',\n",
    "        8: 'Agustos',\n",
    "        9: 'Eylul',\n",
    "        10: 'Ekim',\n",
    "        11: 'Kasim',\n",
    "        12: 'Aralik'\n",
    "    }\n",
    "    \n",
    "    return my_dict.get(month_index)\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance in kilometers between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles. Determines return value units.\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(z)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haversine_distance = F.udf(lambda lon1, lat1, lon2, lat2: haversine(lon1, lat1, lon2, lat2), FloatType())\n",
    "spark.udf.register(\"haversine_distance\", haversine_distance)\n",
    "\n",
    "switch_month = F.udf(lambda z: switch_month_day(z), StringType())\n",
    "spark.udf.register(\"switch_month\", switch_month)\n",
    "\n",
    "switch_tr = F.udf(lambda z: switch_tr_day(z), StringType())\n",
    "spark.udf.register(\"switch_tr\", switch_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"pickup_datetime\",\n",
    "                                   F.to_timestamp(F.col(\"pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "            .withColumn(\"dropoff_datetime\",\n",
    "                        F.to_timestamp(F.col(\"dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"pickup_year\",\n",
    "                        F.year(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "            .withColumn(\"pickup_month\",\n",
    "                        F.month(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "            .withColumn(\"pickup_dayofweek\",\n",
    "                        F.dayofweek(F.to_date(F.col(\"pickup_datetime\")))) \\\n",
    "            .withColumn(\"pickup_hour\",\n",
    "                        F.hour(F.col(\"pickup_datetime\"))) \\\n",
    "            .withColumn(\"pickupDayofWeek_TR\",\n",
    "                        switch_tr(F.col(\"pickup_dayofweek\"))) \\\n",
    "            .withColumn(\"pickupMonth_TR\",\n",
    "                        switch_month(F.col(\"pickup_month\"))) \\\n",
    "            .withColumn(\"haversine_distance(km)\",\n",
    "                        haversine_distance(F.col(\"pickup_longitude\"), F.col(\"pickup_latitude\"),\n",
    "                                           F.col(\"dropoff_longitude\"),\n",
    "                                           F.col(\"dropoff_latitude\"))) \\\n",
    "            .withColumn(\"travel_speed\", \n",
    "                        1000 * F.col(\"haversine_distance(km)\") / F.col(\"trip_duration\")) \\\n",
    "            .drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 1) / 1]\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>pickup_year</th>\n",
       "      <th>pickup_month</th>\n",
       "      <th>pickup_dayofweek</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickupDayofWeek_TR</th>\n",
       "      <th>pickupMonth_TR</th>\n",
       "      <th>haversine_distance(km)</th>\n",
       "      <th>travel_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id2875421</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.982155</td>\n",
       "      <td>40.767937</td>\n",
       "      <td>-73.964630</td>\n",
       "      <td>40.765602</td>\n",
       "      <td>N</td>\n",
       "      <td>455</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Pazartesi</td>\n",
       "      <td>Mart</td>\n",
       "      <td>1.498521</td>\n",
       "      <td>3.293452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id2377394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.980415</td>\n",
       "      <td>40.738564</td>\n",
       "      <td>-73.999481</td>\n",
       "      <td>40.731152</td>\n",
       "      <td>N</td>\n",
       "      <td>663</td>\n",
       "      <td>2016</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Pazar</td>\n",
       "      <td>Haziran</td>\n",
       "      <td>1.805507</td>\n",
       "      <td>2.723239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id3858529</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.979027</td>\n",
       "      <td>40.763939</td>\n",
       "      <td>-74.005333</td>\n",
       "      <td>40.710087</td>\n",
       "      <td>N</td>\n",
       "      <td>2124</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>Salı</td>\n",
       "      <td>Ocak</td>\n",
       "      <td>6.385098</td>\n",
       "      <td>3.006167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id3504673</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.010040</td>\n",
       "      <td>40.719971</td>\n",
       "      <td>-74.012268</td>\n",
       "      <td>40.706718</td>\n",
       "      <td>N</td>\n",
       "      <td>429</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>Çarşamba</td>\n",
       "      <td>Nisan</td>\n",
       "      <td>1.485498</td>\n",
       "      <td>3.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id2181028</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.973053</td>\n",
       "      <td>40.793209</td>\n",
       "      <td>-73.972923</td>\n",
       "      <td>40.782520</td>\n",
       "      <td>N</td>\n",
       "      <td>435</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>Cumartesi</td>\n",
       "      <td>Mart</td>\n",
       "      <td>1.188589</td>\n",
       "      <td>2.732387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  vendor_id  passenger_count  pickup_longitude  pickup_latitude  \\\n",
       "0  id2875421          2                1        -73.982155        40.767937   \n",
       "1  id2377394          1                1        -73.980415        40.738564   \n",
       "2  id3858529          2                1        -73.979027        40.763939   \n",
       "3  id3504673          2                1        -74.010040        40.719971   \n",
       "4  id2181028          2                1        -73.973053        40.793209   \n",
       "\n",
       "   dropoff_longitude  dropoff_latitude store_and_fwd_flag  trip_duration  \\\n",
       "0         -73.964630         40.765602                  N            455   \n",
       "1         -73.999481         40.731152                  N            663   \n",
       "2         -74.005333         40.710087                  N           2124   \n",
       "3         -74.012268         40.706718                  N            429   \n",
       "4         -73.972923         40.782520                  N            435   \n",
       "\n",
       "   pickup_year  pickup_month  pickup_dayofweek  pickup_hour  \\\n",
       "0         2016             3                 2           17   \n",
       "1         2016             6                 1            0   \n",
       "2         2016             1                 3           11   \n",
       "3         2016             4                 4           19   \n",
       "4         2016             3                 7           13   \n",
       "\n",
       "  pickupDayofWeek_TR pickupMonth_TR  haversine_distance(km)  travel_speed  \n",
       "0          Pazartesi           Mart                1.498521      3.293452  \n",
       "1              Pazar        Haziran                1.805507      2.723239  \n",
       "2               Salı           Ocak                6.385098      3.006167  \n",
       "3           Çarşamba          Nisan                1.485498      3.462700  \n",
       "4          Cumartesi           Mart                1.188589      2.732387  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_dayofweek: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickupDayofWeek_TR: string (nullable = true)\n",
      " |-- pickupMonth_TR: string (nullable = true)\n",
      " |-- haversine_distance(km): float (nullable = true)\n",
      " |-- travel_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_subset = df3.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- vendor_id: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- trip_duration: integer (nullable = true)\n",
      " |-- pickup_year: integer (nullable = true)\n",
      " |-- pickup_month: integer (nullable = true)\n",
      " |-- pickup_dayofweek: integer (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- pickupDayofWeek_TR: string (nullable = true)\n",
      " |-- pickupMonth_TR: string (nullable = true)\n",
      " |-- haversine_distance(km): float (nullable = true)\n",
      " |-- travel_speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_subset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_index =  {\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "          \"custom_analyzer\":\n",
    "          {\n",
    "            \"type\":\"custom\",\n",
    "            \"tokenizer\":\"standard\",\n",
    "            \"filter\":[\n",
    "              \"lowercase\", \"custom_edge_ngram\",\"asciifolding\"\n",
    "            ]\n",
    "          }\n",
    "        },\n",
    "        \"filter\": {\n",
    "          \"custom_edge_ngram\": {\n",
    "            \"type\": \"edge_ngram\",\n",
    "            \"min_gram\":2,\n",
    "            \"max_gram\": 20\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": { \"type\": \"text\" },  \n",
    "      \"vendor_id\":  { \"type\": \"integer\"  },\n",
    "      \"passenger_count\": {\"type\":   \"integer\"},\n",
    "      \"pickup_longitude\": {\"type\": \"double\"},\n",
    "      \"pickup_latitude\": {\"type\": \"double\"},\n",
    "      \"pickup_location\": { \"type\": \"geo_point\" },\n",
    "      \"dropoff_longitude\": {\"type\": \"double\"},\n",
    "      \"dropoff_latitude\": {\"type\": \"double\"},\n",
    "      \"store_and_fwd_flag\":    { \"type\": \"keyword\" },\n",
    "      \"trip_duration\":  { \"type\": \"integer\"  }, \n",
    "      \"pickup_year\":   { \"type\": \"integer\"  },\n",
    "      \"pickup_month\": {\"type\": \"integer\"},\n",
    "      \"pickup_dayofweek\": {\"type\":   \"integer\"},\n",
    "      \"pickup_hour\": {\"type\":   \"integer\"},\n",
    "      \"pickupDayofWeek_TR\": {\"type\": \"keyword\"},\n",
    "      \"pickupMonth_TR\": {\"type\": \"keyword\"},\n",
    "      \"haversine_distance(km)\": {\"type\": \"float\"},\n",
    "      \"travel_speed\": {\"type\": \"double\"}\n",
    "    }\n",
    "  }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5212/178024638.py:7: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.delete(\"nyc-taxi-spark\")\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "\n",
    "try:\n",
    "    es.indices.delete(\"nyc-taxi-spark\")\n",
    "    print(\"nyc-taxi-spark  index deleted.\")\n",
    "except:\n",
    "    print(\"No index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5212/3399969818.py:1: DeprecationWarning: The 'body' parameter is deprecated for the 'create' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(\"nyc-taxi-spark\", body=nyc_taxi_index)\n",
      "/tmp/ipykernel_5212/3399969818.py:1: DeprecationWarning: Using positional arguments for APIs is deprecated and will be disabled in 8.0.0. Instead use only keyword arguments for all APIs. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  es.indices.create(\"nyc-taxi-spark\", body=nyc_taxi_index)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nyc-taxi-spark'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(\"nyc-taxi-spark\", body=nyc_taxi_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                  (0 + 2) / 2]\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/manual/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 14.80552363395691 secs -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "taxi_subset.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"es.nodes\", \"localhost\") \\\n",
    "    .option(\"es.port\",\"9200\") \\\n",
    "    .save(\"nyc-taxi-spark\")\n",
    "\n",
    "\n",
    "print(\"----- %s secs -----\" %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f1184234711a9995bb2467cce7b552174d112ea8dbc6085e4c224c89a655f36"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('venvspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
